# -Deep-Learning-for-Audio-with-Python

## 01. Artificial Neuron Implementation
- **Goal:** Understand the basic building block of neural networks.
- **Process:**
  1. Receive multiple inputs.
  2. Apply weights to each input.
  3. Calculate the weighted sum.
  4. Use Sigmoid activation function to squeeze the result between 0 and 1.
- Weighted Sum: $h = \sum w_i x_i$
- Sigmoid: $y = \frac{1}{1 + e^{-h}}$
- **Tools:** Python `math` module.

## 02. Multilayer Perceptron Implementation
- **Goal:** ì—¬ëŸ¬ ì€ë‹‰ì¸µì„ ê°€ì§„ ì‹ ê²½ë§ êµ¬ì¡° ì„¤ê³„ ë° ìˆœì „íŒŒ êµ¬í˜„.
- **Process:**
  1. ì…ë ¥ì¸µ, ì€ë‹‰ì¸µ ë¦¬ìŠ¤íŠ¸, ì¶œë ¥ì¸µì„ í¬í•¨í•œ ì „ì²´ ë ˆì´ì–´ êµ¬ì¡° ì •ì˜.
  2. `numpy.random.randn`ì„ ì´ìš©í•´ ê° ì¸µ ì‚¬ì´ì˜ ê°€ì¤‘ì¹˜ í–‰ë ¬ ì´ˆê¸°í™”.
  3. `numpy.dot` í–‰ë ¬ ê³±ì…ˆì„ í†µí•´ ì¸µê°„ ë°ì´í„° íë¦„ êµ¬í˜„.
  4. ë°˜ë³µë¬¸ì„ í†µí•´ ê° ì¸µì— Sigmoid í™œì„±í™” í•¨ìˆ˜ ì ìš© ë° ìµœì¢… ê²°ê³¼ ë„ì¶œ.
- **Tools:** Python `NumPy`.

## 02-1. Structure Simulation

ë³¸ í”„ë¡œì íŠ¸ì˜ ë™ì‘ ì›ë¦¬ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ 2(ì…ë ¥)-2(ì€ë‹‰)-1(ì¶œë ¥) ë ˆì´ì–´ êµ¬ì¡°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•œ ì—°ì‚° ê³¼ì •ì„ ì •ë¦¬í•©ë‹ˆë‹¤.

### 1. ì „ì œ ì¡°ê±´ (Notation)
* **Activations:** $a_0$ (ì…ë ¥ì¸µ), $a_1$ (ì€ë‹‰ì¸µ), $a_2$ (ì¶œë ¥ì¸µ)
* **Weights:** $W_0$ (ì…ë ¥-ì€ë‹‰ ê°€ì¤‘ì¹˜), $W_1$ (ì€ë‹‰-ì¶œë ¥ ê°€ì¤‘ì¹˜)
* **Functions:** $\sigma$ (Sigmoid í™œì„±í™” í•¨ìˆ˜), $\sigma'$ (Sigmoid ë¯¸ë¶„)
* **Target:** $y$ (ì‹¤ì œ ì •ë‹µ)

---

### 2. ìˆœì „íŒŒ (Forward Propagation)
ë°ì´í„°ê°€ ì…ë ¥ì¸µì—ì„œ ì¶œë ¥ì¸µìœ¼ë¡œ íë¥´ë©° ì˜ˆì¸¡ê°’ $a_2$ë¥¼ ë„ì¶œí•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.

1.  **ì€ë‹‰ì¸µ ê³„ì‚°:**
    $$h_1 = a_0 \cdot W_0$$
    $$a_1 = \sigma(h_1)$$
2.  **ì¶œë ¥ì¸µ ê³„ì‚°:**
    $$h_2 = a_1 \cdot W_1$$
    $$a_2 = \sigma(h_2)$$



---

### 3. ì—­ì „íŒŒ (Backpropagation)
ì˜¤ì°¨ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë’¤ì—ì„œë¶€í„° ê° ì¸µì˜ ê¸°ì—¬ë„(Delta)ë¥¼ ê³„ì‚°í•˜ê³  ê°€ì¤‘ì¹˜ë¥¼ ìˆ˜ì •í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.

#### **Step 1: ì¶œë ¥ì¸µ (Output Layer)**
1.  **ì¶œë ¥ì¸µ ì—ëŸ¬ ($error_2$):** ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ì˜ ì°¨ì´
    $$error_2 = y - a_2$$
2.  **ì¶œë ¥ì¸µ ë¸íƒ€ ($\delta_2$):** ì—ëŸ¬ì— ì¶œë ¥ì¸µ ê¸°ìš¸ê¸°ë¥¼ ì ìš© (Hadamard product)
    $$\delta_2 = error_2 \odot \sigma'(a_2)$$
3.  **ê°€ì¤‘ì¹˜ $W_1$ ìˆ˜ì •ì•ˆ ($Deriv_1$):**
    $$Deriv_1 = a_1^T \cdot \delta_2$$

#### **Step 2: ì€ë‹‰ì¸µ (Hidden Layer)**
1.  **ì€ë‹‰ì¸µ ì—ëŸ¬ ($error_1$):** ì¶œë ¥ì¸µ ë¸íƒ€ê°€ ê°€ì¤‘ì¹˜ë¥¼ íƒ€ê³  ì—­ì „íŒŒë¨
    $$error_1 = \delta_2 \cdot W_1^T$$
2.  **ì€ë‹‰ì¸µ ë¸íƒ€ ($\delta_1$):** ë°°ë‹¬ëœ ì—ëŸ¬ì— ì€ë‹‰ì¸µ ê¸°ìš¸ê¸°ë¥¼ ì ìš© (Hadamard product)
    $$\delta_1 = error_1 \odot \sigma'(a_1)$$
3.  **ê°€ì¤‘ì¹˜ $W_0$ ìˆ˜ì •ì•ˆ ($Deriv_0$):**
    $$Deriv_0 = a_0^T \cdot \delta_1$$



---

### 4. ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ (Weight Update)
ê³„ì‚°ëœ ë¯¸ë¶„ê°’($Deriv$)ê³¼ í•™ìŠµë¥ ($\eta$)ì„ ì‚¬ìš©í•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ ì‹¤ì œë¡œ ìˆ˜ì •í•©ë‹ˆë‹¤.

* $W_1 = W_1 + \eta \cdot Deriv_1$
* $W_0 = W_0 + \eta \cdot Deriv_0$

---

### ğŸ’¡ í•µì‹¬ ì›ë¦¬ ìš”ì•½
* **$\delta$ (Delta):** ê° ì¸µì˜ ë‰´ëŸ°ì´ ê²°ê³¼ì— ëŒ€í•´ ì±…ì„ì ¸ì•¼ í•  **ì˜¤ì°¨ì˜ ë³¸ì²´**ì…ë‹ˆë‹¤. í•­ìƒ `error âŠ™ f'(a)`ì˜ ì¼ê´€ëœ í˜•íƒœë¡œ ê³„ì‚°ë©ë‹ˆë‹¤.
* **$\cdot$ (Dot Product):** ì—ëŸ¬ ì‹ í˜¸ë¥¼ ì• ì¸µìœ¼ë¡œ **ì „ë‹¬**í•˜ê±°ë‚˜, ê°€ì¤‘ì¹˜ ì „ì²´ì˜ **ìˆ˜ì • ì§€ë„**ë¥¼ ê·¸ë¦´ ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
* **$\odot$ (Hadamard Product):** í•´ë‹¹ ì¸µì˜ í™œì„±í™” í•¨ìˆ˜ íŠ¹ì„±(ê¸°ìš¸ê¸°)ì„ ì—ëŸ¬ì— **í•„í„°ë§**í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

### ğŸ”¢ ìˆ˜ì¹˜ ê¸°ë°˜ ì‹œë®¬ë ˆì´ì…˜ (Numerical Example)

2-2-1 êµ¬ì¡°ì—ì„œ ì‹¤ì œ ìˆ«ìê°€ ì–´ë–»ê²Œ ê³„ì‚°ë˜ê³  ì „ë‹¬ë˜ëŠ”ì§€ ë‹¨ê³„ë³„ë¡œ ì‚´í´ë´…ë‹ˆë‹¤.

#### **1. ìˆœì „íŒŒ (Forward)**
* **Input ($a_0$):** $[1.0, 2.0]$
* **Weights ($W_1$):** ëª¨ë‘ $0.5$ë¡œ ê°€ì • (ì€ë‹‰-ì¶œë ¥ ê°€ì¤‘ì¹˜)
* **Target ($y$):** $1.0$ (ëª©í‘œ ì •ë‹µ)
* **Output ($a_2$):** $0.68$ (ì˜ˆì¸¡ê°’), **Hidden ($a_1$):** $[0.6, 0.7]$

#### **2. ì—­ì „íŒŒ (Backward) - ì¶œë ¥ì¸µ**
1.  **$error_2$ ê³„ì‚°:**
    $$error_2 = y - a_2 = 1.0 - 0.68 = \mathbf{0.32}$$
2.  **$\delta_2$ ê³„ì‚° (ì˜¤ì°¨ $\odot$ ê¸°ìš¸ê¸°):**
    * $\sigma'(a_2) = 0.68 \times (1 - 0.68) \approx 0.22$
    $$\delta_2 = error_2 \odot \sigma'(a_2) = 0.32 \times 0.22 = \mathbf{0.07}$$
3.  **$Deriv_1$ ì‘ì„± (ì´ì „ ì¸µ ì¶œë ¥ $a_1^T$ì™€ ë¸íƒ€ $\delta_2$ì˜ ë‚´ì ):**
```math
Deriv_1 = \begin{bmatrix} 0.6 \\ 0.7 \end{bmatrix} \cdot [0.07] = \begin{bmatrix} 0.042 \\ 0.049 \end{bmatrix}


#### **3. ì—­ì „íŒŒ (Backward) - ì€ë‹‰ì¸µ**
1.  **$error_1$ ì „ë‹¬ (ì—ëŸ¬ ë¦´ë ˆì´):**
    * $\delta_2$ê°€ ê°€ì¤‘ì¹˜ $W_1$ì„ íƒ€ê³  ì—­í–‰
    $$error_1 = \delta_2 \cdot W_1^T = [0.07] \cdot [0.5, 0.5] = \mathbf{[0.035, 0.035]}$$
2.  **$\delta_1$ ê³„ì‚° (ë°°ë‹¬ëœ ì—ëŸ¬ $\odot$ ì€ë‹‰ì¸µ ê¸°ìš¸ê¸°):**
    * $\sigma'(a_1) = [0.6(1-0.6), \; 0.7(1-0.7)] = [0.24, 0.21]$
    $$\delta_1 = error_1 \odot \sigma'(a_1) = [0.035, 0.035] \odot [0.24, 0.21] = \mathbf{[0.0084, 0.00735]}$$
3.  **$Deriv_0$ ì‘ì„± (ì…ë ¥ê°’ $a_0^T$ì™€ ë¸íƒ€ $\delta_1$ì˜ ë‚´ì ):**
    $$Deriv_0 = \begin{bmatrix} 1.0 \\ 2.0 \end{bmatrix} \cdot [0.0084, 0.00735]$$
    $$Deriv_0 = \mathbf{\begin{bmatrix} 0.0084 & 0.00735 \\ 0.0168 & 0.0147 \end{bmatrix}}$$
